# OpenAI SDK ä»ç½‘é¡µAIè§†è§’å­¦ä¹ æŒ‡å—

æœ¬æ–‡æ¡£ä»¥ä½ ç†Ÿæ‚‰çš„ç½‘é¡µAIå’ŒClaude Codeä½¿ç”¨ä½“éªŒä¸ºèµ·ç‚¹ï¼Œé€æ­¥ä»‹ç»å¦‚ä½•ç”¨SDKå®ç°åŒæ ·çš„åŠŸèƒ½ã€‚

## å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬æ–‡æ¡£ï¼Œä½ å°†å­¦ä¼šï¼š
- å¦‚ä½•ç”¨SDKå®ç°ç½‘é¡µAIçš„åŸºæœ¬å¯¹è¯åŠŸèƒ½
- å¦‚ä½•å®ç°æ–°ä¼šè¯ã€æ€è€ƒæ¨¡å¼ã€è”ç½‘ç­‰é«˜çº§åŠŸèƒ½
- å¦‚ä½•è®©AIè°ƒç”¨å·¥å…·ã€è®¿é—®æ–‡ä»¶ç­‰

## çŸ¥è¯†åŸºç¡€

å‡è®¾ä½ å·²ç»å…·å¤‡ï¼š
1. ç†Ÿæ‚‰ç½‘é¡µAIçš„åŸºæœ¬æ“ä½œï¼ˆChatGPTã€Claudeç­‰ï¼‰
2. äº†è§£Claude Codeçš„åŸºæœ¬åŠŸèƒ½
3. æœ‰åŸºæœ¬çš„ç¼–ç¨‹çŸ¥è¯†ï¼ˆPython/JavaScriptï¼‰
4. äº†è§£å¼‚æ­¥ç¼–ç¨‹çš„åŸºæœ¬æ¦‚å¿µ

## å­¦ä¹ è·¯å¾„

### 1. åŸºç¡€å¯¹è¯åŠŸèƒ½ï¼ˆç½‘é¡µAIçš„åŸºç¡€ä½“éªŒï¼‰

#### 1.1 ä»ç½‘é¡µå¯¹è¯åˆ°SDKä»£ç 

**ä½ åœ¨ç½‘é¡µAIä¸­çš„ä½“éªŒ**ï¼š
```
ä½ : å¸®æˆ‘å†™ä¸€ä¸ªPythonçš„Hello Worldç¨‹åº
AI: å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„Python Hello Worldç¨‹åºï¼š
    print("Hello, World!")
```

**ç”¨SDKå®ç°åŒæ ·çš„å¯¹è¯**ï¼š

```python
from openai import OpenAI

# 1. åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆç›¸å½“äºæ‰“å¼€ç½‘é¡µAIï¼‰
client = OpenAI(
    api_key="ä½ çš„APIå¯†é’¥",
    base_url="https://api.openai.com"  # æˆ–è€…å…¶ä»–APIæœåŠ¡
)

# 2. å‘é€æ¶ˆæ¯ï¼ˆç›¸å½“äºåœ¨è¾“å…¥æ¡†æ‰“å­—å¹¶å‘é€ï¼‰
response = client.chat.completions.create(
    model="gpt-4",  # é€‰æ‹©AIæ¨¡å‹
    messages=[
        {"role": "user", "content": "å¸®æˆ‘å†™ä¸€ä¸ªPythonçš„Hello Worldç¨‹åº"}
    ]
)

# 3. è·å–å›å¤ï¼ˆç›¸å½“äºçœ‹åˆ°AIçš„å›ç­”ï¼‰
ai_response = response.choices[0].message.content
print(ai_response)
```

**å…³é”®ç†è§£**ï¼š
- `client.chat.completions.create()` ç›¸å½“äºç‚¹å‡»"å‘é€"æŒ‰é’®
- `messages` æ•°ç»„å°±æ˜¯ä½ ä»¬çš„å¯¹è¯å†å²
- `response.choices[0].message.content` å°±æ˜¯AIçš„å›ç­”

#### 1.2 å®ç°æ‰“å­—æœºæ•ˆæœï¼ˆæµå¼å“åº”ï¼‰

**ç½‘é¡µAIçš„ä½“éªŒ**ï¼šAIçš„å›ç­”æ˜¯ä¸€ä¸ªå­—ä¸€ä¸ªå­—å‡ºç°çš„ï¼Œå°±åƒæœ‰äººåœ¨æ‰“å­—

**ç”¨SDKå®ç°æ‰“å­—æœºæ•ˆæœ**ï¼š

```python
# æ·»åŠ  stream=True å‚æ•°
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "}
    ],
    stream=True  # å…³é”®å‚æ•°ï¼šå¯ç”¨æµå¼å“åº”
)

# é€ä¸ªå­—ç¬¦æ¥æ”¶AIçš„å›ç­”
full_response = ""
for chunk in response:
    # æ¯ä¸ªchunkåŒ…å«ä¸€å°æ®µæ–‡å­—
    delta = chunk.choices[0].delta

    if delta.content:
        # ç´¯åŠ æ”¶åˆ°çš„æ–‡å­—ç‰‡æ®µ
        full_response += delta.content
        print(delta.content, end='', flush=True)  # å®æ—¶æ˜¾ç¤º

print("\nå®Œæ•´å›ç­”:", full_response)
```

**è¿™æ ·ä½ å°±èƒ½çœ‹åˆ°AIåƒæ‰“å­—ä¸€æ ·é€å­—å›ç­”é—®é¢˜äº†ï¼**

#### 1.3 å¤šè½®å¯¹è¯ï¼ˆä¿æŒä¸Šä¸‹æ–‡ï¼‰

**ç½‘é¡µAIçš„ä½“éªŒ**ï¼šä½ å¯ä»¥åŸºäºä¹‹å‰çš„å¯¹è¯ç»§ç»­æé—®

**ç”¨SDKå®ç°å¤šè½®å¯¹è¯**ï¼š

```python
# å¯¹è¯å†å²ï¼ˆå°±åƒç½‘é¡µä¸­çš„èŠå¤©è®°å½•ï¼‰
messages = [
    {"role": "user", "content": "æˆ‘å«å¼ ä¸‰"},
    {"role": "assistant", "content": "ä½ å¥½å¼ ä¸‰ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ï¼"},
    {"role": "user", "content": "ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ"}
]

response = client.chat.completions.create(
    model="gpt-4",
    messages=messages  # åŒ…å«å®Œæ•´å¯¹è¯å†å²
)

print(response.choices[0].message.content)
# AIä¼šå›ç­”ï¼šå½“ç„¶è®°å¾—ï¼Œä½ å«å¼ ä¸‰ï¼
```

**ç†è§£è¦ç‚¹**ï¼š
- æ¯æ¬¡å¯¹è¯éƒ½è¦åŒ…å«å®Œæ•´çš„å†å²è®°å½•
- `messages` æ•°ç»„å°±æ˜¯AIçš„"è®°å¿†"
- è¶Šé•¿çš„å†å²è®°å½•ï¼Œæ¶ˆè€—çš„tokenè¶Šå¤š

### 2. ä¼šè¯ç®¡ç†ï¼ˆæ–°ä¼šè¯ã€å†å²è®°å½•ï¼‰

#### 2.1 æ–°ä¼šè¯ vs å»¶ç»­ä¼šè¯

**ç½‘é¡µAIçš„ä½“éªŒ**ï¼š
- ç‚¹å‡»"æ–°å¯¹è¯"å¼€å§‹å…¨æ–°çš„ä¼šè¯
- åœ¨æ—§å¯¹è¯ä¸­ç»§ç»­èŠå¤©ä¿æŒä¸Šä¸‹æ–‡

**ç”¨SDKå®ç°ä¼šè¯ç®¡ç†**ï¼š

```python
class ChatSession:
    def __init__(self, client, system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹"):
        self.client = client
        self.messages = [{"role": "system", "content": system_prompt}]

    def new_conversation(self):
        """å¼€å§‹æ–°ä¼šè¯"""
        self.messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹"}]
        print("âœ¨ æ–°ä¼šè¯å·²å¼€å§‹")

    def chat(self, user_input):
        """å‘é€æ¶ˆæ¯å¹¶è·å–å›å¤"""
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.messages.append({"role": "user", "content": user_input})

        # è·å–AIå›å¤
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=self.messages
        )

        ai_response = response.choices[0].message.content

        # æ·»åŠ AIå›å¤åˆ°å†å²
        self.messages.append({"role": "assistant", "content": ai_response})

        return ai_response

    def get_history(self):
        """è·å–å¯¹è¯å†å²"""
        return self.messages.copy()

# ä½¿ç”¨ç¤ºä¾‹
client = OpenAI(api_key="your-key")
session = ChatSession(client)

# ç¬¬ä¸€æ¬¡å¯¹è¯
print(session.chat("æˆ‘å«æå››"))  # AIä¼šè®°ä½è¿™ä¸ªåå­—

# ç»§ç»­å¯¹è¯
print(session.chat("æˆ‘åˆšæ‰å‘Šè¯‰ä½ ä»€ä¹ˆåå­—ï¼Ÿ"))  # AIä¼šå›ç­”ï¼šæå››

# å¼€å§‹æ–°ä¼šè¯
session.new_conversation()
print(session.chat("æˆ‘åˆšæ‰å‘Šè¯‰ä½ ä»€ä¹ˆåå­—ï¼Ÿ"))  # AIä¼šè¯´ä¸è®°å¾—
```

#### 2.2 ä¼šè¯æŒä¹…åŒ–ï¼ˆä¿å­˜å’ŒåŠ è½½å¯¹è¯ï¼‰

**ä¿å­˜å¯¹è¯å†å²åˆ°æ–‡ä»¶**ï¼š

```python
import json

def save_session(session, filename="chat_history.json"):
    """ä¿å­˜ä¼šè¯åˆ°æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(session.get_history(), f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ å¯¹è¯å·²ä¿å­˜åˆ° {filename}")

def load_session(client, filename="chat_history.json"):
    """ä»æ–‡ä»¶åŠ è½½ä¼šè¯"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            messages = json.load(f)

        session = ChatSession(client)
        session.messages = messages
        print(f"ğŸ“‚ å·²ä» {filename} åŠ è½½å¯¹è¯å†å²")
        return session
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
        return ChatSession(client)

# ä½¿ç”¨ç¤ºä¾‹
session = ChatSession(client)
session.chat("æ¨èå‡ æœ¬Pythonå…¥é—¨ä¹¦ç±")
save_session(session)

# ç¨åç»§ç»­å¯¹è¯
loaded_session = load_session(client)
print(loaded_session.chat("åˆšæ‰æ¨èçš„ä¹¦ä¸­æœ‰å“ªæœ¬æœ€é€‚åˆåˆå­¦è€…ï¼Ÿ"))
```

### 3. é«˜çº§åŠŸèƒ½ï¼ˆæ€è€ƒæ¨¡å¼ã€è”ç½‘ã€å·¥å…·è°ƒç”¨ï¼‰

#### 3.1 æ€è€ƒæ¨¡å¼ï¼ˆReasoningï¼‰

**ç½‘é¡µAIçš„ä½“éªŒ**ï¼šæŸäº›AIä¼šæ˜¾ç¤º"æ­£åœ¨æ€è€ƒ..."çš„è¿‡ç¨‹

**ç”¨SDKå®ç°æ€è€ƒæ¨¡å¼**ï¼š

```python
# ä½¿ç”¨æ”¯æŒæ¨ç†çš„æ¨¡å‹ï¼ˆå¦‚DeepSeek Reasonerï¼‰
response = client.chat.completions.create(
    model="deepseek-reasoner",
    messages=[
        {"role": "user", "content": "è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†"}
    ],
    stream=True
)

reasoning_content = ""
final_content = ""

for chunk in response:
    delta = chunk.choices[0].delta

    if delta.reasoning_content:
        # AIæ­£åœ¨æ€è€ƒ
        reasoning_content += delta.reasoning_content
        print(f"ğŸ¤” æ€è€ƒä¸­: {delta.reasoning_content}", end='')
    elif delta.content:
        # AIç»™å‡ºæœ€ç»ˆç­”æ¡ˆ
        final_content += delta.content
        print(f"ğŸ’¡ å›ç­”: {delta.content}", end='')

print(f"\n\nå®Œæ•´æ€è€ƒè¿‡ç¨‹:\n{reasoning_content}")
print(f"\næœ€ç»ˆç­”æ¡ˆ:\n{final_content}")
```

#### 3.2 è”ç½‘åŠŸèƒ½ï¼ˆè·å–æœ€æ–°ä¿¡æ¯ï¼‰

**ç½‘é¡µAIçš„ä½“éªŒ**ï¼šAIå¯ä»¥è®¿é—®äº’è”ç½‘è·å–æœ€æ–°ä¿¡æ¯

**ç”¨SDKå®ç°è”ç½‘åŠŸèƒ½**ï¼š

```python
import requests
from datetime import datetime

def search_web(query):
    """æ¨¡æ‹Ÿç½‘ç»œæœç´¢"""
    # è¿™é‡Œå¯ä»¥ä½¿ç”¨çœŸå®çš„æœç´¢å¼•æ“API
    return f"æœç´¢ç»“æœï¼šå…³äº'{query}'çš„æœ€æ–°ä¿¡æ¯ï¼ˆæˆªè‡³{datetime.now()}ï¼‰"

def web_enhanced_chat(client, user_input):
    """å¸¦è”ç½‘åŠŸèƒ½çš„å¯¹è¯"""

    # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦è”ç½‘
    check_response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "åˆ¤æ–­ç”¨æˆ·é—®é¢˜æ˜¯å¦éœ€è¦æœç´¢æœ€æ–°ä¿¡æ¯ã€‚å¦‚æœéœ€è¦ï¼Œå›ç­”'éœ€è¦æœç´¢'ï¼›å¦åˆ™å›ç­”'ä¸éœ€è¦æœç´¢'"},
            {"role": "user", "content": user_input}
        ]
    )

    needs_search = "éœ€è¦æœç´¢" in check_response.choices[0].message.content

    messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"}]

    if needs_search:
        # 2. æ‰§è¡Œç½‘ç»œæœç´¢
        search_result = search_web(user_input)
        print(f"ğŸŒ æ­£åœ¨æœç´¢: {user_input}")

        messages.append({
            "role": "system",
            "content": f"åŸºäºä»¥ä¸‹æœç´¢ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š{search_result}"
        })

    # 3. è·å–æœ€ç»ˆå›ç­”
    messages.append({"role": "user", "content": user_input})

    response = client.chat.completions.create(
        model="gpt-4",
        messages=messages
    )

    return response.choices[0].message.content

# ä½¿ç”¨ç¤ºä¾‹
print(web_enhanced_chat(client, "ä»Šå¤©åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ"))  # ä¼šè§¦å‘æœç´¢
print(web_enhanced_chat(client, "è§£é‡Šä»€ä¹ˆæ˜¯Pythonï¼Ÿ"))  # ä¸éœ€è¦æœç´¢
```

#### 3.3 å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰

**ç½‘é¡µAIçš„ä½“éªŒ**ï¼šAIå¯ä»¥è°ƒç”¨è®¡ç®—å™¨ã€æŸ¥è¯¢æ•°æ®åº“ç­‰å·¥å…·

**ç”¨SDKå®ç°å·¥å…·è°ƒç”¨**ï¼š

```python
import math
import json
import requests

# å®šä¹‰AIå¯ä»¥ä½¿ç”¨çš„å·¥å…·
tools = [
    {
        "type": "function",
        "function": {
            "name": "calculate",
            "description": "æ‰§è¡Œæ•°å­¦è®¡ç®—",
            "parameters": {
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "è¦è®¡ç®—çš„æ•°å­¦è¡¨è¾¾å¼"
                    }
                },
                "required": ["expression"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°"
                    }
                },
                "required": ["city"]
            }
        }
    }
]

# å·¥å…·å®ç°å‡½æ•°
def calculate(expression):
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    try:
        # å®‰å…¨çš„æ•°å­¦è¡¨è¾¾å¼æ±‚å€¼ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        result = eval(expression)
        return {"result": result}
    except:
        return {"error": "æ— æ³•è®¡ç®—è¯¥è¡¨è¾¾å¼"}

def get_weather(city):
    """è·å–å¤©æ°”ä¿¡æ¯"""
    # è¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„å¤©æ°”API
    return {"city": city, "temperature": "25Â°C", "weather": "æ™´"}

def tool_assisted_chat(client, user_input):
    """å¸¦å·¥å…·è°ƒç”¨çš„å¯¹è¯"""

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å›ç­”é—®é¢˜"},
        {"role": "user", "content": user_input}
    ]

    response = client.chat.completions.create(
        model="gpt-4",
        messages=messages,
        tools=tools,  # æä¾›å¯ç”¨å·¥å…·
        tool_choice="auto"  # è®©AIè‡ªåŠ¨é€‰æ‹©å·¥å…·
    )

    message = response.choices[0].message

    # æ£€æŸ¥AIæ˜¯å¦è¦è°ƒç”¨å·¥å…·
    if message.tool_calls:
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        tool_results = []
        for tool_call in message.tool_calls:
            function_name = tool_call.function.name
            arguments = json.loads(tool_call.function.arguments)

            if function_name == "calculate":
                result = calculate(arguments["expression"])
            elif function_name == "get_weather":
                result = get_weather(arguments["city"])
            else:
                result = {"error": "æœªçŸ¥å·¥å…·"}

            tool_results.append({
                "tool_call_id": tool_call.id,
                "role": "tool",
                "name": function_name,
                "content": json.dumps(result)
            })

        # å°†å·¥å…·è°ƒç”¨å’Œç»“æœåŠ å…¥å¯¹è¯
        messages.append(message)
        messages.extend(tool_results)

        # è·å–åŸºäºå·¥å…·ç»“æœçš„æœ€ç»ˆå›ç­”
        final_response = client.chat.completions.create(
            model="gpt-4",
            messages=messages
        )

        return final_response.choices[0].message.content

    else:
        # AIæ²¡æœ‰è°ƒç”¨å·¥å…·ï¼Œç›´æ¥å›ç­”
        return message.content

# ä½¿ç”¨ç¤ºä¾‹
print(tool_assisted_chat(client, "è®¡ç®— 123 * 456"))  # ä¼šè°ƒç”¨è®¡ç®—å™¨å·¥å…·
print(tool_assisted_chat(client, "åŒ—äº¬ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ"))  # ä¼šè°ƒç”¨å¤©æ°”å·¥å…·
print(tool_assisted_chat(client, "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "))  # ä¸ä¼šè°ƒç”¨å·¥å…·
```

#### 3.4 æ–‡ä»¶è®¿é—®èƒ½åŠ›

**ç½‘é¡µAIçš„ä½“éªŒ**ï¼šAIå¯ä»¥è¯»å–å’Œåˆ†æä½ ä¸Šä¼ çš„æ–‡ä»¶

**ç”¨SDKå®ç°æ–‡ä»¶è®¿é—®**ï¼š

```python
import os
from pathlib import Path

class FileAssistant:
    def __init__(self, client, workspace_dir="./workspace"):
        self.client = client
        self.workspace_dir = Path(workspace_dir)
        self.workspace_dir.mkdir(exist_ok=True)

    def get_available_files(self):
        """è·å–å·¥ä½œç©ºé—´ä¸­çš„æ–‡ä»¶åˆ—è¡¨"""
        files = []
        for file_path in self.workspace_dir.rglob("*"):
            if file_path.is_file():
                relative_path = file_path.relative_to(self.workspace_dir)
                files.append(str(relative_path))
        return files

    def read_file(self, filename):
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        file_path = self.workspace_dir / filename
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            return f"é”™è¯¯ï¼šæ–‡ä»¶ {filename} ä¸å­˜åœ¨"

    def write_file(self, filename, content):
        """å†™å…¥æ–‡ä»¶"""
        file_path = self.workspace_dir / filename
        file_path.parent.mkdir(parents=True, exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return f"æ–‡ä»¶ {filename} å·²ä¿å­˜"

    def chat_with_files(self, user_input):
        """å¯ä»¥è®¿é—®æ–‡ä»¶çš„å¯¹è¯"""

        # æ„å»ºå·¥å…·å®šä¹‰
        file_tools = [
            {
                "type": "function",
                "function": {
                    "name": "list_files",
                    "description": "åˆ—å‡ºå·¥ä½œç©ºé—´ä¸­çš„æ‰€æœ‰æ–‡ä»¶",
                    "parameters": {"type": "object", "properties": {}}
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "read_file",
                    "description": "è¯»å–æŒ‡å®šæ–‡ä»¶çš„å†…å®¹",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "filename": {"type": "string", "description": "è¦è¯»å–çš„æ–‡ä»¶å"}
                        },
                        "required": ["filename"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "write_file",
                    "description": "åˆ›å»ºæˆ–ä¿®æ”¹æ–‡ä»¶",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "filename": {"type": "string", "description": "æ–‡ä»¶å"},
                            "content": {"type": "string", "description": "æ–‡ä»¶å†…å®¹"}
                        },
                        "required": ["filename", "content"]
                    }
                }
            }
        ]

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¯ä»¥è®¿é—®å’Œä¿®æ”¹æ–‡ä»¶çš„AIåŠ©æ‰‹ã€‚å·¥ä½œç©ºé—´ç›®å½•ï¼š" + str(self.workspace_dir)},
            {"role": "user", "content": user_input}
        ]

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=messages,
            tools=file_tools,
            tool_choice="auto"
        )

        message = response.choices[0].message

        # å¤„ç†å·¥å…·è°ƒç”¨
        if message.tool_calls:
            tool_results = []

            for tool_call in message.tool_calls:
                function_name = tool_call.function.name
                arguments = json.loads(tool_call.function.arguments)

                if function_name == "list_files":
                    result = {"files": self.get_available_files()}
                elif function_name == "read_file":
                    result = {"content": self.read_file(arguments["filename"])}
                elif function_name == "write_file":
                    result = {"message": self.write_file(arguments["filename"], arguments["content"])}
                else:
                    result = {"error": "æœªçŸ¥å·¥å…·"}

                tool_results.append({
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": function_name,
                    "content": json.dumps(result, ensure_ascii=False)
                })

            # ç»§ç»­å¯¹è¯
            messages.append(message)
            messages.extend(tool_results)

            final_response = self.client.chat.completions.create(
                model="gpt-4",
                messages=messages
            )

            return final_response.choices[0].message.content

        return message.content

# ä½¿ç”¨ç¤ºä¾‹
file_assistant = FileAssistant(client)

# å…ˆåˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ–‡ä»¶
file_assistant.write_file("example.py", "print('Hello, World!')")

# ä¸æ–‡ä»¶è¿›è¡Œå¯¹è¯
print(file_assistant.chat_with_files("å·¥ä½œç©ºé—´ä¸­æœ‰å“ªäº›æ–‡ä»¶ï¼Ÿ"))
print(file_assistant.chat_with_files("è¯»å– example.py æ–‡ä»¶çš„å†…å®¹"))
print(file_assistant.chat_with_files("åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶ notes.mdï¼Œå†…å®¹æ˜¯ä»Šå¤©çš„å­¦ä¹ ç¬”è®°"))
```

### 4. å®é™…é¡¹ç›®åº”ç”¨ï¼ˆkube-aiä¸­çš„å®ç°ï¼‰

ç°åœ¨æˆ‘ä»¬çŸ¥é“äº†å¦‚ä½•ç”¨SDKå®ç°ç½‘é¡µAIçš„å„ç§åŠŸèƒ½ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨kube-aié¡¹ç›®ä¸­åº”ç”¨è¿™äº›çŸ¥è¯†ã€‚

#### 4.1 kube-aiçš„æ ¸å¿ƒæ¶æ„

```typescript
// kube-aiä¸­çš„ä¸»è¦ç»„ä»¶
class KubeAI {
    private openai: OpenAI;
    private workspace: AIWorkspace;
    private gitManager: GitManager;

    constructor() {
        this.openai = new OpenAI({
            apiKey: process.env.OPENAI_API_KEY
        });
        this.workspace = new AIWorkspace();
        this.gitManager = new GitManager();
    }

    async startNewConversation(userRequest: string): Promise<void> {
        // 1. åˆ›å»ºæ–°åˆ†æ”¯
        const branchName = `feature/${Date.now()}`;
        await this.gitManager.createBranch(branchName);

        // 2. å¼€å§‹AIå¯¹è¯
        const conversation = this.workspace.createConversation();

        // 3. æ„å»ºä¸“ä¸šæç¤ºè¯
        const systemPrompt = `
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Kubernetesä¸“å®¶ã€‚
        è¯·æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚ç”Ÿæˆæ ‡å‡†çš„Kubernetes YAMLé…ç½®æ–‡ä»¶ã€‚
        æ‰€æœ‰é…ç½®æ–‡ä»¶éƒ½åº”è¯¥éµå¾ªK8sæœ€ä½³å®è·µã€‚
        `;

        // 4. å®šä¹‰AIå·¥å…·
        const tools = [
            {
                name: "create_k8s_deployment",
                description: "åˆ›å»ºKubernetes Deployment",
                parameters: {
                    name: { type: "string", description: "åº”ç”¨åç§°" },
                    image: { type: "string", description: "Dockeré•œåƒ" },
                    replicas: { type: "number", description: "å‰¯æœ¬æ•°é‡" }
                }
            },
            {
                name: "create_k8s_service",
                description: "åˆ›å»ºKubernetes Service",
                parameters: {
                    name: { type: "string", description: "æœåŠ¡åç§°" },
                    type: { type: "string", description: "æœåŠ¡ç±»å‹" },
                    port: { type: "number", description: "ç«¯å£" }
                }
            },
            {
                name: "commit_changes",
                description: "æäº¤æ–‡ä»¶æ›´æ”¹åˆ°Git",
                parameters: {
                    message: { type: "string", description: "æäº¤ä¿¡æ¯" }
                }
            }
        ];

        // 5. æ‰§è¡ŒAIå¯¹è¯
        await this.executeAIConversation(conversation, systemPrompt, userRequest, tools);
    }

    private async executeAIConversation(
        conversation: AIConversation,
        systemPrompt: string,
        userRequest: string,
        tools: any[]
    ): Promise<void> {
        const messages = [
            { role: "system", content: systemPrompt },
            { role: "user", content: userRequest }
        ];

        const response = await this.openai.chat.completions.create({
            model: "gpt-4",
            messages: messages,
            tools: tools,
            tool_choice: "auto",
            stream: true
        });

        let fullResponse = "";
        let currentToolCall = null;

        for await (const chunk of response) {
            const delta = chunk.choices[0].delta;

            // å¤„ç†å†…å®¹æµ
            if (delta.content) {
                fullResponse += delta.content;
                // å®æ—¶æ›´æ–°UIæ˜¾ç¤ºAIæ­£åœ¨æ€è€ƒ
                this.updateUI({ type: "thinking", content: fullResponse });
            }

            // å¤„ç†å·¥å…·è°ƒç”¨
            if (delta.tool_calls) {
                for (const toolCall of delta.tool_calls) {
                    if (toolCall.function) {
                        await this.executeTool(toolCall.function);
                    }
                }
            }
        }

        // ä¿å­˜å¯¹è¯å†å²
        conversation.addMessage("assistant", fullResponse);
    }

    private async executeTool(functionCall: any): Promise<void> {
        const { name, arguments: args } = functionCall;

        switch (name) {
            case "create_k8s_deployment":
                const deploymentYaml = this.generateDeploymentYAML(args);
                await this.workspace.writeFile("deployment.yaml", deploymentYaml);
                await this.gitManager.add("deployment.yaml");
                break;

            case "create_k8s_service":
                const serviceYaml = this.generateServiceYAML(args);
                await this.workspace.writeFile("service.yaml", serviceYaml);
                await this.gitManager.add("service.yaml");
                break;

            case "commit_changes":
                await this.gitManager.commit(args.message);
                this.updateUI({ type: "git_committed", message: args.message });
                break;
        }
    }
}
```

#### 4.2 å®æ—¶UIæ›´æ–°

```typescript
// å‰ç«¯UIæ›´æ–°é€»è¾‘
function updateUI(update: any) {
    switch (update.type) {
        case "thinking":
            document.getElementById("ai-thinking").textContent = update.content;
            break;

        case "file_created":
            document.getElementById("file-list").innerHTML +=
                `<div class="file-item">âœ… ${update.filename}</div>`;
            break;

        case "git_committed":
            document.getElementById("git-status").textContent = `âœ… ${update.message}`;
            break;

        case "yaml_generated":
            // å®æ—¶æ›´æ–°YAMLç¼–è¾‘å™¨
            monacoEditor.setValue(update.content);
            break;
    }
}
```

## å­¦ä¹ èµ„æºå’Œå®éªŒ

### å®éªŒä»£ç ç»“æ„

```bash
lab/openai-sdk/
â”œâ”€â”€ basic-chat.py          # åŸºç¡€å¯¹è¯åŠŸèƒ½
â”œâ”€â”€ session-management.py  # ä¼šè¯ç®¡ç†
â”œâ”€â”€ tools-demo.py         # å·¥å…·è°ƒç”¨ç¤ºä¾‹
â”œâ”€â”€ file-assistant.py     # æ–‡ä»¶è®¿é—®åŠŸèƒ½
â””â”€â”€ kube-ai-demo.ts       # kube-aiæ ¸å¿ƒé€»è¾‘
```

### å¿«é€Ÿå¼€å§‹

1. **å®‰è£…ä¾èµ–**ï¼š
```bash
pip install openai python-dotenv
```

2. **é…ç½®APIå¯†é’¥**ï¼š
```bash
export OPENAI_API_KEY="your-api-key"
```

3. **è¿è¡ŒåŸºç¡€ç¤ºä¾‹**ï¼š
```bash
python lab/openai-sdk/basic-chat.py
```

### å®è·µç»ƒä¹ 

1. **åŸºç¡€ç»ƒä¹ **ï¼šå®ç°ä¸€ä¸ªç®€å•çš„èŠå¤©æœºå™¨äºº
2. **è¿›é˜¶ç»ƒä¹ **ï¼šæ·»åŠ ä¼šè¯å†å²ç®¡ç†
3. **é«˜çº§ç»ƒä¹ **ï¼šå®ç°å·¥å…·è°ƒç”¨åŠŸèƒ½
4. **é¡¹ç›®å®è·µ**ï¼šä¸ºkube-aiæ·»åŠ æ–°çš„AIå·¥å…·

## æ€»ç»“

é€šè¿‡ä»ç½‘é¡µAIçš„ç†Ÿæ‚‰ä½“éªŒå‡ºå‘ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **åŸºç¡€å¯¹è¯**ï¼š`client.chat.completions.create()` å®ç°ç½‘é¡µèŠå¤©
2. **æµå¼å“åº”**ï¼š`stream=True` å®ç°æ‰“å­—æœºæ•ˆæœ
3. **ä¼šè¯ç®¡ç†**ï¼š`messages` æ•°ç»„ç»´æŠ¤å¯¹è¯å†å²
4. **é«˜çº§åŠŸèƒ½**ï¼šå·¥å…·è°ƒç”¨å®ç°è®¡ç®—ã€æœç´¢ã€æ–‡ä»¶è®¿é—®
5. **å®é™…åº”ç”¨**ï¼šåœ¨kube-aiä¸­æ„å»ºä¸“ä¸šçš„K8sé…ç½®åŠ©æ‰‹

ç°åœ¨ä½ å·²ç»å…·å¤‡äº†ç”¨SDKå®ç°ç½‘é¡µAIæ‰€æœ‰åŠŸèƒ½çš„èƒ½åŠ›ï¼ä¸‹ä¸€æ­¥å°±æ˜¯æ ¹æ®å…·ä½“éœ€æ±‚ï¼Œè®¾è®¡å±äºä½ è‡ªå·±çš„AIåº”ç”¨äº†ã€‚